---
title: "ETL Pipeline for PGA Statistics"
date: 2020-06-29
tags: [golf stats, data science, etl, web scraping, databases]
header: 
excerpt: "Golf Analysis, Data Science, ETL, Web Scraping, MongoDB" 
mathjax: "true"
---

# Introduction  
In a previous post I attempted to validate an age old saying in golf. The results were not quite what I expected, so naturally I decided to see whether new data could posssibly improve my analysis. In this post I will create a web scraper to extract my own data for future projects. 

**You can find the files specified in this post on my [Github profile](https://github.com/EBogdanov2/DataSci_Projects/tree/master/PGA)**

# Set Up
The first component I needed was a method for storing the data that would be collected. To achieve this I decided to go with a MongoDB database instead of a SQL based database, for no reason other than learning NoSQL database programming. For the data collection I decided to use the Scrapy library for Python in conjunction with the PyMongo library.

# Data 
The data was scraped from the [official PGA site](https://www.pgatour.com), specifically the data poriton of the site. I only plan on using this data for project purposees in the future and currently do not plan on a major scraping operation.

# Starting the Project

The first task was to simply start the project
```python 
$ scrapy startproject pga_etl 
```

After navigaitng to the **items.py** file I defined the statistics I would be extracting. 

```python
import scrapy

from scrapy.item import Item, Field

# Define the stats that are to be extracted for the database
class PgaEtlItem(Item):
    name = Field() # player name
    rank = Field() # rank for specified stat
    distance = Field() # distance off the tee
    tournament = Field() # tournament statistic was recorded in
    pass
``` 

# Spider creation

After setting up the project, I proceeded to create a Spider in order to define how to scrape and extract data from a set of links provided. For simplicity I decided to only scrape a specific page off the site as a proof of concept, but plan to quickly expand this scraper to cycle through the multiple pages of statistics. The file is named **pga_etl_spider.py**

```python
from scrapy import Spider 
from scrapy.selector import Selector 

from pga_etl.items import PgaEtlItem

# Define the Spider class for PGA stat extraction
class PgaEtlSpider(Spider):
    name = "pga" 
    allowed_domains = ["pgatour.com"] # define the official PGA site
    start_urls = ["https://www.pgatour.com/content/pgatour/stats/stat.159.y2020.eon.t525.html"] # Define the page containing the off the tee stats for 3M Open  

    def parse(self, response):
        stats = Selector(response).xpath('//*[@id="statsTable"]//tbody//tr') # XML of the table containing the stats to be extracted

        for stat in stats: # Extract the data within the table
            statistics = PgaEtlItem()
            statistics['rank'] = stat.xpath('td[1]/text()').extract()[0]
            statistics['name'] = stat.xpath('td[3]/a/text()').extract()[0]
            statistics['distance'] = stat.xpath('td[5]/text()').extract()[0] 
            
            statistics['tournament'] = "3M Open" # For simplicity this is manually entered, later iterations will cycle through multiple tournamnets
            yield statistics
``` 

# MongoDB Integration

Now that the data has been extracted, all that is left is to store it. First I added a couple of settings into the **settings.py** file to define where on the system the database is located and what to name it.

```python 
ITEM_PIPELINES = {'pga_etl.pipelines.PgaEtlPipeline': 300}

MONGODB_SERVER = "localhost"
MONGODB_PORT = 27017
MONGODB_DB = "pga_data"
MONGODB_COLLECTION = "stats"
``` 

Following the settings definition, I created **pipelines.py**. This is the final component allowing the information extracted from the Spider class previopusly defined to be stored in a local database. 

```python 
import pymongo 
import logging

from scrapy.utils.project import get_project_settings 
from scrapy.exceptions import DropItem

from itemadapter import ItemAdapter

settings = get_project_settings() 

# Create the pipeline class for the database
class PgaEtlPipeline: 
    def __init__(self):
        # Connect to the database outlined in settings.py
        connection = pymongo.MongoClient(
            settings['MONGODB_SERVER'],
            settings['MONGODB_PORT']
        )
        db = connection[settings['MONGODB_DB']]
        self.collection = db[settings['MONGODB_COLLECTION']]

    # Add the scraped items from the Spider class into the database
    def process_item(self, item, spider): 
        valid = True
        for data in item: 
            if not data: 
                valid = False 
                raise DropItem("Missing {0}!".format(data))
            if valid: 
                self.collection.insert(dict(item))
                logging.log(msg="Stat added to the database",level = logging.DEBUG, spider = spider)
        return item
```

# Results

The final results were 68 total extracted documents. All the rows from the scraped table were succesfully added to the database with no errors. This is a good start, from here I aim to expand the scraper to more tables in order to extract a complete data set condusive to analysis. 